#!/bin/bash
#set -x
#[TODO{Sidra}:INFRA-1072}]

source /home/$USER/.aurora.conf
. /opt/pg/scripts/gc_helpers.sh
. /opt/pg/scripts/aurora_infra_settings.conf
function show_build_help() {
    cat << BUILDHELP
USAGE: ./nimbus -l <BUILD-ID>

Required Arguments:
  -l, --buildid         Build on which you want to run test.

Optional Arguments:
  -W, --WORKSPACE       Set the workspace where all the logs will be stored.
                        Logs will be place in /tmp/logs
  -t, --tag             Add a tag to the instance name. The tag will get a suffix of director, edge
                        or workload. There should be no other instances with TAG<suffix>. Default is nimbus
  -c, --commit          SHA-1 HASH of the commit to apply
                        Supplying this arguments enables the transfer of local changes to the cloud instance.
                        The changes can be specified by the SHA-1 hash of a local commit.
                        All changes up to that commit are applied on the instance on the corresponding
                        branch.
  -D, --directors       Number of PG directors to spawn. Default is 1
  -E, --edges           Number of PG edges to spawn. Default is 2
  -N, --neutron         To deploy neutron instance or not. Default is 0
  -A, --apex            To deploy CloudApex or not. Default is 0
  -Ss, --start-step     Step number to start execution at. Default is 1
  -Se, --end-step       Step numbet to end execution at. Default is 4
  --start-plumgrid      Start plumgrid services on directors and edges
  --stop-plumgrid       Stop plumgrid services on directors and edges
  --state-dir           Please provide full path of Nimbus state directory if using --start-step other than 1
  --test-cleanup        Cleanup test cruft. Usually used if a test failed
  --debug               Run in debug mode. Do not delete log files
BUILDHELP
}

STEP_ST=1
STEP_ED=5
NUM_DIRS=1
NUM_EDGES=2
NEUTRON=0
APEX=0
TAG="n"
DATE=$(date +"%Y%m%d%H%M%S")
username="plumgrid"

while true ; do
  case "$1" in
     -l| --buildid) export BUILD_NAME=$2 ; shift 2 ;;
     -t| --tag) export TAG=$2 ; shift 2 ;;
     -c| --commit) export COMMIT=$2 ; shift 2 ;;
     -W| --WORKSPACE) export WORKSPACE=$2 ; shift 2 ;;
     -D| --directors) export NUM_DIRS=$2 ; shift 2 ;;
     -E| --edges) export NUM_EDGES=$2 ; shift 2 ;;
     -N| --neutron) export NEUTRON=1 ; shift ;;
     -A| --apex) export APEX=1 ; shift ;;
     -Ss| --start-step) export STEP_ST=$2 ; shift 2 ;;
     -Se| --end-step) export STEP_ED=$2 ; shift 2 ;;
     --start-plumgrid) export START_PLUMGRID=1 ; shift ;;
     --stop-plumgrid) export STOP_PLUMGRID=1 ; shift ;;
     --test-cleanup) export TEST_CLEANUP=1 ; shift ;;
     --state-dir) export state_dir=$2 ; shift 2 ;;
     --debug) export debug=1 ; shift ;;
     -h| --help ) show_build_help; exit 0; shift;;
     --) shift ; break ;;
     *) break;;
  esac
done

if [[ -z "${WORKSPACE}" ]]; then
  export WORKSPACE=/tmp/nimbus_run_$DATE
  if [[ ! -d $WORKSPACE/logs ]]; then
    mkdir -p $WORKSPACE/logs
  fi
else
  export WORKSPACE=$WORKSPACE/nimbus_run_$DATE
  if [[ ! -d $WORKSPACE/logs ]]; then
    mkdir -p $WORKSPACE/logs
  fi
fi

if [[ -z "${state_dir}" ]]; then
  state_dir=$(mktemp -d ${WORKSPACE}/nimbus_conf-XXXXX)
else
  rm -rf ${WORKSPACE}
  WORKSPACE=$(dirname ${state_dir})
fi

if [[ -z "$BUILD_NAME" ]]; then
  echo "BUILD_NAME cannot be empty. Please specify an existing build name with -l"
  exit 1
else
  aurora ls build | grep -q "${BUILD_NAME}"
  if [[ $? -ne 0 ]]; then
    echo "Build does not exist. Please use aurora ls."
    exit 1
  fi
fi

#Append tag if provided
if [[ $TAG != "none" ]]; then
  #Only keeping alphanumeric characters
  TAG=${TAG//[^[:alnum:]]/}
  #Converting to lowercase since instance name cannot contain uppercase characters
  TAG=${TAG,,}
fi

function start_plumgrid(){
  directors=$(aurora ls instances | grep ".*${TAG}director" | awk '{print $3}')
  directors_array=($directors)

  for i in "${directors_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo start plumgrid && sudo start nginx" &
  done
  wait
  if [[ $? -ne 0 ]]; then
    echo "Starting plumgrid services on directors failed"
    exit 1
  fi


  edges=$(aurora ls instances | grep ".*${TAG}edge" | awk '{print $3}')
  edges_array=($edges)

  for i in "${edges_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo start plumgrid-iovisor" &
  done
  wait
  if [[ $? -ne 0 ]]; then
    echo "Starting plumgrid services on edges failed"
    exit 1
  fi
  exit 0
}

function stop_plumgrid(){
  directors=$(aurora ls instances | grep ".*${TAG}director" | awk '{print $3}')
  directors_array=($directors)

  for i in "${directors_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo stop plumgrid && sudo stop nginx" &
  done
  wait

  if [[ $? -ne 0 ]]; then
    echo "Stopping plumgrid services on directors failed"
    exit 1
  fi

  edges=$(aurora ls instances | grep ".*${TAG}edge" | awk '{print $3}')
  edges_array=($edges)

  for i in "${edges_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo stop plumgrid-iovisor" &
  done
  wait
  if [[ $? -ne 0 ]]; then
    echo "Stopping plumgrid services on edges failed"
    exit 1
  fi
  exit 0
}

function test_cleanup(){
  directors=$(aurora ls instances | grep ".*${TAG}director" | awk '{print $3}')
  directors_array=($directors)

  for i in "${directors_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo /opt/pg/systest/infra/test-cleanup.sh" &
  done
  wait

  if [[ $? -ne 0 ]]; then
    echo "Cleaning up directors failed."
    exit 1
  fi

  edges=$(aurora ls instances | grep ".*${TAG}edge" | awk '{print $3}')
  edges_array=($edges)

  for i in "${edges_array[@]}"; do
    ssh -o StrictHostKeyChecking=no $username@$i "sudo sudo /opt/pg/systest/infra/test-cleanup.sh" &
  done
  wait
  if [[ $? -ne 0 ]]; then
    echo "Cleaning up edges failed"
    exit 1
  fi
  exit 0
}

if [[ "$START_PLUMGRID" = "1" ]]; then
  start_plumgrid
fi
if [[ "$STOP_PLUMGRID" = "1" ]]; then
  stop_plumgrid
fi
if [[ "$TEST_CLEANUP" = "1" ]]; then
  test_cleanup
fi

#cost code for for falcon is 14, ref: pl_settings.conf
aurora_run_command="aurora run -l ${BUILD_NAME} --cost_pipeline_code 14"
if [[ ! -z $COMMIT ]]; then
  aurora_run_command+=" -c $COMMIT"
fi

function launch_vm_and_conf() {
  local TYPE=$1
  local TAG=$2
  local NUM=$3
  local date_begin=$(date)
  echo "$TAG"
  echo "================================="


  case "$TYPE" in
  "DIR")
    local_file=$(mktemp ${WORKSPACE}/launch-dir-conf-XXXXX)
    local local_log_file=$WORKSPACE/logs/`basename $local_file`
    echo "$date_begin:: Creating command file $local_file Logs will be stored in $local_log_file.*.log"
    cat > $local_file <<DELIM__
#!/bin/bash
set -x
function copy_wk_key_to_MW(){
  echo "COPYING WORKLOAD KEY TO MW"
  /usr/bin/expect <<EOD
set timeout 30
spawn ssh -o StrictHostKeyChecking=no plumgrid@\$middleware_ip "echo \"\$workload_key\" >> ~/.ssh/authorized_keys"
expect "password:"
send "plumgrid\n"
expect eof
EOD
}

for i in {1..$NUM}; do
  $aurora_run_command -t ${TAG}director >& $local_log_file.dir\$i.log &
done
$aurora_run_command -t ${TAG}edge >& $local_log_file.edge1.log &
$aurora_run_command -t ${TAG}workload >& $local_log_file.workload.log &

wait
if [[ \$? -ne 0 ]]; then
  echo "$date_begin:: Encountered error while spawning ${NUM} directors and master edge1 with tag ${TAG}" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
mkdir -p $state_dir/workload_keys
ssh-keygen -t rsa -b 2048 -f '$state_dir/workload_keys/id_rsa' -N ''

wlname=\$(grep -o "Warning: Permanently added '.*'" $local_log_file.workload.log | head -n1)
if [[ "x\$wlname" == "x" ]]; then
  echo "$date_begin:: Unable to grep IP address for workload VM with $TAG tag" >> /tmp/nimbus.log
  exit 1
fi
if [[ "\$wlname" =~ ^Warning:\ Permanently\ added\ \'([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})\'$ ]]; then
  wlip=\${BASH_REMATCH[1]}
fi

echo "\$wlip" > $state_dir/workload_ip

# DIRECTOR CONF
if [[ -z $state_dir/workload_keys/id_rsa.pub ]]; then
  echo "$date_begin:: Unable to read $state_dir/workload_keys/id_rsa.pub during $TYPE VM with $TAG tag configuration" >> $WORKSPACE/logs/nimbus.log
fi
workload_key=\$(cat $state_dir/workload_keys/id_rsa.pub)

directors=\$(aurora ls instances | grep ".*${TAG}director" | awk '{print \$3}')
directors_array=(\$directors)

dir_ips=\$(echo \$directors | tr " " ",")
echo "\$dir_ips" > $state_dir/dir_ips

virtual_ip=\$(aurora ls instances | grep ".*${TAG}director" | awk '{print \$3}' | head -1)
echo "\$virtual_ip" > $state_dir/virtual_ip

edge1_log_file=\$(ls ${WORKSPACE}/logs/*.edge1.log)
edgename=\$(grep "Creating the instance\[.*\] in the cloud" \$edge1_log_file | head -n1)
if [[ "x\$edgename" == "x" ]]; then
  echo "$date_begin:: Unable to extract edge1 instance name" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
if [[ "\$edgename" =~ ^Creating.the.instance\[(.*)\].in.the.cloud$ ]]; then
  edge1_instance_name=\${BASH_REMATCH[1]}
else
  echo "$date_begin:: Unable to extract edge1 instance name" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
echo "\$edge1_instance_name" > $state_dir/edge1_instance_name
edgename=\$(grep -o "Warning: Permanently added '.*'" \$edge1_log_file | head -n1)
if [[ "x\$edgename" == "x" ]]; then
  echo "$date_begin:: Unable to extract IP address for edge1" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
if [[ "\$edgename" =~ ^Warning:\ Permanently\ added\ \'([0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3})\'$ ]]; then
  edgeip=\${BASH_REMATCH[1]}
else
  echo "$date_begin:: Unable to extract IP address for edge1" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
echo "\$edgeip" > $state_dir/edge1_ip
[[ -s $state_dir/edge_ips.lock ]] && rm -rf $state_dir/edge_ips.lock
[[ -s $state_dir/edge_ips ]] && rm -rf $state_dir/edge_ips
echo -n "\$edgeip" > $state_dir/edge_ips


for i in "\${directors_array[@]}"; do
  scp -o StrictHostKeyChecking=no /opt/pg/scripts/nimbus_remote_conf $username@\$i:/opt/pg/scripts/. >& /tmp/nimbus_remote_conf.log &
done
scp -o StrictHostKeyChecking=no /opt/pg/scripts/nimbus_remote_conf $username@\$edgeip:/opt/pg/scripts/. >& /tmp/nimbus_remote_conf.log &
scp -o StrictHostKeyChecking=no /opt/pg/scripts/nimbus_remote_conf $username@\$wlip:/opt/pg/scripts/. >& /tmp/nimbus_remote_conf.log &

scp -o StrictHostKeyChecking=no $state_dir/workload_keys/* $username@\$wlip:~/.ssh/. >& /tmp/nimbus_remote_conf.log &
wait

for i in "\${directors_array[@]}"; do
  ssh -o StrictHostKeyChecking=no $username@\$i "sudo /opt/pg/scripts/nimbus_remote_conf \"DIR\" \$dir_ips \$virtual_ip $middleware_ip >& /tmp/nimbus_remote_conf.log" &
  ssh -o StrictHostKeyChecking=no $username@\$i "echo \"\$workload_key\" >> ~/.ssh/authorized_keys" &
done
ssh -o StrictHostKeyChecking=no $username@\$edgeip "sudo /opt/pg/scripts/nimbus_remote_conf \"EDGE\" \$dir_ips \$virtual_ip $middleware_ip >& /tmp/nimbus_remote_conf.log" &
ssh -o StrictHostKeyChecking=no $username@\$edgeip "echo \"\$workload_key\" >> ~/.ssh/authorized_keys" &

#Copy workload key to middleware vm
if [[ "$APEX" -eq "1" ]]; then
  echo "COPYING WORKLOAD KEY TO MW"
  copy_wk_key_to_MW
fi
wait
if [[ \$? -ne 0 ]]; then
  echo "$date_begin:: Unable to configure directors and master edge1 with ${TAG} tag" >> $WORKSPACE/logs/nimbus.log
  exit 1
fi
DELIM__

    ;;

  "EDGE")
    edge_clone_file=$(mktemp ${WORKSPACE}/clone-edge-XXXXX)
    local local_log_file=$WORKSPACE/logs/`basename $edge_clone_file`
    echo "$date_begin:: Creating command file $local_file Logs will be stored in $local_log_file.*.log"
    cat > $edge_clone_file <<DELIM__
#!/bin/bash
set -x
. /opt/pg/scripts/gc_helpers.sh
. /opt/pg/scripts/aurora_infra_settings.conf
INSTANCE_NAME=\$1
NUM=\$2
echo "Creating the disks[\${INSTANCE_NAME}\${NUM}-d1/d2] for edge clone \${NUM}"
tryexec exec_gcloud_cmd disks create "\${INSTANCE_NAME}\${NUM}-d1" -q \
  --source-snapshot "\${INSTANCE_NAME}-ssd1" --type "$DISK_TYPE" --size="$DISK1_SIZE" &
tryexec exec_gcloud_cmd disks create "\${INSTANCE_NAME}\${NUM}-d2" -q \
  --source-snapshot "\${INSTANCE_NAME}-ssd2" --type "$DISK_TYPE" --size="$DISK2_SIZE" &
wait

key_file=\$(mktemp $WORKSPACE/keys-XXXXX)
echo plumgrid:\$(cat /home/${USER}/.ssh/id_rsa.pub) > \${key_file}
echo plumgrid:\$(echo ${COMMON_KEY}) >> \${key_file}
echo plumgrid:\$(cat $state_dir/workload_pub_key) >> \${key_file}
echo "Creating the instance[\$INSTANCE_NAME] in the cloud"
tryexec exec_gcloud_cmd instances create "\${INSTANCE_NAME}\${NUM}" -q\
  --machine-type "$MACHINE_TYPE" --network "$NETWORK" --maintenance-policy \
  "MIGRATE" --scopes "$SCOPES" \
  --disk "name=\${INSTANCE_NAME}\${NUM}-d1" "mode=rw" "boot=yes" "auto-delete=yes" \
  --disk "name=\${INSTANCE_NAME}\${NUM}-d2" "mode=rw" "boot=no" "auto-delete=yes" --no-address --tags "no-ip" \
  --metadata-from-file sshKeys=\$key_file
rm -rf \$key_file

INSTANCE_IP=\$(get_instance_ip \${INSTANCE_NAME}\${NUM})
echo "Waiting for the machine to boot up and allow ssh access"
tryexec wait_for_instance \${INSTANCE_IP}

echo "Changing MTU on the instance"
tryexec run_cmd_gci \${INSTANCE_IP} "sudo ifconfig eth0 mtu 1400"
tryexec run_cmd_gci \${INSTANCE_IP} "DT=\$(ifconfig eth0 | grep 'inet addr' | cut -d: -f2|cut -d' ' -f 1); sudo bash -c 'echo \"'\$DT'  \`hostname\`\" >> /etc/hosts'"
if [[ -s $state_dir/edge_ips ]]; then
  while [[ -f $state_dir/edge_ips.lock ]]; do
    sleep 1
  done
  touch $state_dir/edge_ips.lock
  echo -n ",\$INSTANCE_IP" >> $state_dir/edge_ips
  rm $state_dir/edge_ips.lock
else
  while [[ -f $state_dir/edge_ips.lock ]]; do
    sleep 1
  done
  touch $state_dir/edge_ips.lock
  echo -n "\$INSTANCE_IP" > $state_dir/edge_ips
  rm $state_dir/edge_ips.lock
fi
DELIM__
    ;;

  *)
    echo "NOP launh_vm_and_config"
    ;;

  esac

}

function launch_middleware(){
  # Create Middleware VM if apex flag is checked
  echo "  ===CREATING MIDDLEWARE INSTANCE===" >> $WORKSPACE/logs/nimbus.log

  #Populating Instance name for Middlware VM
  INSTANCE_NAME=$( echo "${emailid}-${BUILD_NAME}-${TAG}middleware" | sed 's/bld/run/g')
  mkdir "$state_dir/middleware_keys"
  echo plumgrid:$(cat /home/${USER}/.ssh/id_rsa.pub) > $state_dir/middleware_keys/tmp.key
  echo "$date_begin:: Deploying ${INSTANCE_NAME}" >> $WORKSPACE/logs/nimbus.log

  #Creating Middleware Instance
  tryexec exec_gcloud_cmd instances create "${INSTANCE_NAME}" -q \
  --image "mw-image" --machine-type "n1-standard-2" --network "$NETWORK" \
  --metadata-from-file sshKeys="$state_dir/middleware_keys/tmp.key"

  export middleware_ip=$(get_instance_ip ${INSTANCE_NAME})
  echo ${middleware_ip} > "$state_dir/middleware_ip"
  echo " ===MIDDLEWARE VM LAUNCHED with IP:${middleware_ip}===" >> $WORKSPACE/logs/nimbus.log
}

function install_middleware_deb(){
  # Create a script that will run on the workload vm
  # It will copy the MW deb to MW vm and then install it.
  mw_install_script_path=$(mktemp ${WORKSPACE}/mw-install-XXXXX.sh)
  mw_script_name=$(basename $mw_install_script_path)
  cat > $mw_install_script_path <<DELIM__
#!/bin/bash
set -x
echo "removing nginx symlinks"
ssh -o StrictHostKeyChecking=no plumgrid@$middleware_ip "sudo rm -f /etc/nginx/sites-enabled/elk*"
echo "removing stale nginx confs"
ssh -o StrictHostKeyChecking=no plumgrid@$middleware_ip "sudo rm -f /etc/nginx/sites-available/elk*"
echo "Copy MW deb to VM"
scp -o StrictHostKeyChecking=no /home/plumgrid/work/middle-earth/build/plumgrid-middleware*.deb plumgrid@$middleware_ip:
echo "Middleware deb copied to VM"
echo "stop middleware service"
ssh -o StrictHostKeyChecking=no plumgrid@$middleware_ip "sudo service plumgrid-middleware stop"
echo "force install deb pkg"
ssh -o StrictHostKeyChecking=no plumgrid@$middleware_ip "sudo dpkg -i --force-all /home/plumgrid/plumgrid-middleware*.deb"
echo "start middleware service"
ssh -o StrictHostKeyChecking=no  plumgrid@$middleware_ip "sudo service plumgrid-middleware start"
DELIM__

  scp -o StrictHostKeyChecking=no $mw_install_script_path $username@$workload:/tmp/. >& /tmp/mw_install_script.log
  ssh -o StrictHostKeyChecking=no $username@$workload "bash /tmp/$mw_script_name >& /tmp/mw_install_script.log"
}

function run_step(){
  local step=$1
  case "$step" in
  1)
    #verify existing insatnces, if none found launch directors.
    # STEP 2 CREATE DIRECTOR VMS
    aurora ls instances | grep -q "$.*${TAG}workload"
    if [[ $? -eq 0 ]]; then
      echo "Workload VM already exists"
      exit 1
    fi
    aurora ls instances | grep -q "$.*${TAG}edge"
    if [[ $? -eq 0 ]]; then
      echo "EDGE VM already exists"
      exit 1
    fi
    aurora ls instances | grep -q ".*${TAG}director"
    if [[ $? -eq 0 ]]; then
      echo "Director already exists"
      exit 1
    fi

    if [[ $APEX = "1" ]]; then
      echo " ===APEX FLAG CHECKED===" >> $WORKSPACE/logs/nimbus.log
      launch_middleware
    fi

    launch_vm_and_conf "DIR" $TAG $NUM_DIRS
    bash $local_file

    if [[ $? -ne 0 ]]; then
      echo "Director spawn and configure failed"
      exit 1
    fi
    ;;

  2)
    # STEP 3 CREATE EDGE VMS

    # Get Edge 1 instance name
    edgeip=$(cat ${state_dir}/edge1_ip)
    edge1_instance_name=$(cat ${state_dir}/edge1_instance_name)

    #snapshot disks for edge1
    tryexec run_cmd_gci $edgeip "sudo sync"
    tryexec run_cmd_gci $edgeip "sudo fsfreeze -f /opt || true"
    tryexec exec_gcloud_cmd disks snapshot $edge1_instance_name-d1 --snapshot-names $edge1_instance_name-ssd1 &
    tryexec exec_gcloud_cmd disks snapshot $edge1_instance_name-d2 --snapshot-names $edge1_instance_name-ssd2 &
    wait
    tryexec run_cmd_gci $edgeip "sudo fsfreeze -u /opt || true"
    ;;

  3)
    # create file that takes in input edge number and snapshot name
    # run this file X times in parallel
    # File should create disks from snapshot and then spawn instances
    MACHINE_TYPE="g1-small"
    if [[ "$NEUTRON" = "1" ]]; then
       MACHINE_TYPE="n1-standard-2"
    fi

    launch_vm_and_conf "EDGE"

    edge1_instance_name=$(cat ${state_dir}/edge1_instance_name)
    for i in `seq 2 $NUM_EDGES`; do
      sem --gnu -u -j 50 bash $edge_clone_file $edge1_instance_name $i>& $WORKSPACE/logs/launch_snapshot_edge$i.log
    done

    sem --gnu --wait
    if [[ $? -ne 0 ]]; then
      echo "Edge spawn and configure failed"
      exit 1
    fi
    ;;

  4)
    # STEP 4 WORKLOAD CONF
    if [[ -z $state_dir/workload_ip ]]; then
      echo "$date_begin:: Unable to ready $state_dir/workload_ip during workload VM with $TAG tag configuration" >> $WORKSPACE/logs/nimbus.log
    fi
    workload=$(cat $state_dir/workload_ip)

    if [[ -z $state_dir/dir_ips ]]; then
      echo "$date_begin:: Unable to ready $state_dir/dir_ips during workload VM with $TAG tag configuration" >> $WORKSPACE/logs/nimbus.log
    fi
    directors=$(cat $state_dir/dir_ips)

    if [[ -z $state_dir/edge_ips ]]; then
      echo "$date_begin:: Unable to ready $state_dir/edge_ips during workload VM with $TAG tag configuration" >> $WORKSPACE/logs/nimbus.log
    fi
    edges=$(cat $state_dir/edge_ips)
    scp -o StrictHostKeyChecking=no /opt/pg/scripts/nimbus_remote_conf $username@$workload:/opt/pg/scripts/. >& /tmp/nimbus_remote_conf.log
    ssh -o StrictHostKeyChecking=no $username@$workload "sudo /opt/pg/scripts/nimbus_remote_conf \"WORKLOAD\" $directors $edges $middleware_ip >& /tmp/nimbus_remote_conf.log"

    #Install latest MW deb on the MW VM
    if [[ $APEX = "1" ]]; then
      echo "Installing middleware deb" >> $WORKSPACE/logs/nimbus.log
      install_middleware_deb
    fi
    ;;

  5)
      #######################################################################################################
      # Neutron Compatibility Guide:
      #------------------------------------------------------------------------------------------------------
      # **Master**      On master branch 4.2 onwards compatibility requires neutron KILO with default
      #                 aurora instance and default user "plumgrid"
      #------------------------------------------------------------------------------------------------------
      # **Stable_4_1**  Till stable_4_1 compatibility requires neutron JUNO with default user "ubuntu"
      ########################################################################################################

    if [[ $NEUTRON = "1" ]]; then
      # STEP 5 LAUNCH NEUTRON SNAPSHOT
      echo "  ===LAUNCHING NEUTRON IN THE ORBIT===">> $WORKSPACE/logs/nimbus.log
      neutron_user="plumgrid"
      neutron_snapshot="neutron-kilo-snapshot"
      neutron_snapshot_size="10GB"

      if [[ $BUILD_NAME = *"stable-4-1"* ]]; then
        echo "******************Deploying Stable 4.1 Compatible Neutron Instance*************** "
        neutron_user="ubuntu"
        neutron_snapshot="neutron-snapshot2"
        neutron_snapshot_size="50GB"
      else
        echo "****************** Deploying Stable 4.2 Onwards Compatible Neutron Instance *************** "
      fi
      #Populating Instance Name for neutron
      INSTANCE_NAME=$( echo "${emailid}-${BUILD_NAME}-${TAG}neutron" | sed 's/bld/run/g')
      #Log entry
      echo "$date_begin:: Deploying ${INSTANCE_NAME}" >> $WORKSPACE/logs/nimbus.log
      mkdir "$state_dir/neutron_keys"
      #Adding Test Machine Public Key for Neutron
      echo ${neutron_user}:$(cat /home/${USER}/.ssh/id_rsa.pub) > $state_dir/neutron_keys/tmp.key
      #Adding Workload Public Key to Neutron
      echo ${neutron_user}:$(cat $state_dir/workload_keys/id_rsa.pub) >> $state_dir/neutron_keys/tmp.key
      #Creating Neutron Disk Instance
      tryexec exec_gcloud_cmd disks create "${INSTANCE_NAME}-d1" -q \
      --source-snapshot ${neutron_snapshot} --type "$DISK_TYPE" --size=${neutron_snapshot_size}


      #Creating Neutron Instance Creation
      tryexec exec_gcloud_cmd instances create "${INSTANCE_NAME}" -q\
      --machine-type "g1-small" --network "$NETWORK" --maintenance-policy "MIGRATE" --scopes "$SCOPES" \
      --disk "name=${INSTANCE_NAME}-d1" "mode=rw" "boot=yes" "auto-delete=yes" --no-address --tags "no-ip" \
      --metadata-from-file sshKeys="$state_dir/neutron_keys/tmp.key"

      neutron_ip=$(get_instance_ip ${INSTANCE_NAME})
      echo ${neutron_ip} > "$state_dir/neutron_ips"
      echo "  ===NEUTRON TO EARTH: IT'S A NEW DAY!===  " >>  $WORKSPACE/logs/nimbus.log       #Log entry
    else
      echo "  ===NEUTRON NOT LAUNCHED!===  " >>  $WORKSPACE/logs/nimbus.log                   #Log entry
    fi
    ;;
  *)
    echo "NOP - $step"
    ;;

  esac

}

script_start_time=$(date +%s)
if [[ ! -d "${WORKSPACE}/logs" ]] ; then
  mkdir -p ${WORKSPACE}/logs
fi

for STEP in `seq ${STEP_ST} ${STEP_ED}`; do
  printf "\n    === Starting NIMBUS STEP $STEP === \n"
  start_time=$(date +%s)
  run_step $STEP
  end_time=$(date +%s)
  print_time_taken $start_time $end_time "[$STEP]"
done
script_end_time=$(date +%s)
print_time_taken $script_start_time $script_end_time "[NIMBUS]"

if [[ -z "${debug}" ]]; then
  echo "Removing $WORKSPACE/logs because --debug flag was not set"
  rm -rf $WORKSPACE/logs
fi
